{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Topsis\n**Comparison using Pre-Trained models**","metadata":{}},{"cell_type":"code","source":"!pip install nltk rouge-score bert-score\nimport nltk\nfrom datasets import load_dataset\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom rouge_score import rouge_scorer\nimport pandas as pd\nfrom bert_score import score\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", message=\"Your max_length is set to *\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T11:16:38.653856Z","iopub.execute_input":"2025-02-02T11:16:38.654647Z","iopub.status.idle":"2025-02-02T11:16:50.807397Z","shell.execute_reply.started":"2025-02-02T11:16:38.654588Z","shell.execute_reply":"2025-02-02T11:16:50.806062Z"},"trusted":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nRequirement already satisfied: rouge-score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: bert-score in /opt/conda/lib/python3.10/site-packages (0.3.13)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.24.4)\nRequirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.1.2+cpu)\nRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.2.0)\nRequirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.37.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bert-score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.10/site-packages (from bert-score) (4.67.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score) (3.7.4)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from bert-score) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->bert-score) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score) (2023.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.0.0->bert-score) (2023.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.15.1)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=3.0.0->bert-score) (0.4.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (1.4.5)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score) (9.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bert-score) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"!pip install py7zr","metadata":{"execution":{"iopub.status.busy":"2025-02-02T11:16:50.810156Z","iopub.execute_input":"2025-02-02T11:16:50.810538Z","iopub.status.idle":"2025-02-02T11:17:03.090270Z","shell.execute_reply.started":"2025-02-02T11:16:50.810505Z","shell.execute_reply":"2025-02-02T11:17:03.088774Z"},"trusted":true},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: py7zr in /opt/conda/lib/python3.10/site-packages (0.22.0)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.7.0)\nRequirement already satisfied: pycryptodomex>=3.16.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (3.21.0)\nRequirement already satisfied: pyzstd>=0.15.9 in /opt/conda/lib/python3.10/site-packages (from py7zr) (0.16.2)\nRequirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.1.1)\nRequirement already satisfied: pybcj<1.1.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.0.3)\nRequirement already satisfied: multivolumefile>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from py7zr) (0.2.3)\nRequirement already satisfied: inflate64<1.1.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.0.1)\nRequirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from py7zr) (1.1.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from py7zr) (5.9.3)\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"# Dataset: SAMSUM","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"samsum\")\ndf= dataset['test'].to_pandas()","metadata":{"execution":{"iopub.status.busy":"2025-02-02T11:17:03.092190Z","iopub.execute_input":"2025-02-02T11:17:03.092576Z","iopub.status.idle":"2025-02-02T11:17:05.407828Z","shell.execute_reply.started":"2025-02-02T11:17:03.092535Z","shell.execute_reply":"2025-02-02T11:17:05.406252Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2025-02-02T11:17:05.409455Z","iopub.execute_input":"2025-02-02T11:17:05.409859Z","iopub.status.idle":"2025-02-02T11:17:05.423045Z","shell.execute_reply.started":"2025-02-02T11:17:05.409831Z","shell.execute_reply":"2025-02-02T11:17:05.421755Z"},"trusted":true},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"         id                                           dialogue  \\\n0  13862856  Hannah: Hey, do you have Betty's number?\\nAman...   \n1  13729565  Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...   \n2  13680171  Lenny: Babe, can you help me with something?\\r...   \n3  13729438  Will: hey babe, what do you want for dinner to...   \n4  13828600  Ollie: Hi , are you in Warsaw\\r\\nJane: yes, ju...   \n\n                                             summary  \n0  Hannah needs Betty's number but Amanda doesn't...  \n1  Eric and Rob are going to watch a stand-up on ...  \n2  Lenny can't decide which trousers to buy. Bob ...  \n3  Emma will be home soon and she will let Will k...  \n4  Jane is in Warsaw. Ollie and Jane has a party....  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>dialogue</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>13862856</td>\n      <td>Hannah: Hey, do you have Betty's number?\\nAman...</td>\n      <td>Hannah needs Betty's number but Amanda doesn't...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>13729565</td>\n      <td>Eric: MACHINE!\\r\\nRob: That's so gr8!\\r\\nEric:...</td>\n      <td>Eric and Rob are going to watch a stand-up on ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13680171</td>\n      <td>Lenny: Babe, can you help me with something?\\r...</td>\n      <td>Lenny can't decide which trousers to buy. Bob ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13729438</td>\n      <td>Will: hey babe, what do you want for dinner to...</td>\n      <td>Emma will be home soon and she will let Will k...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>13828600</td>\n      <td>Ollie: Hi , are you in Warsaw\\r\\nJane: yes, ju...</td>\n      <td>Jane is in Warsaw. Ollie and Jane has a party....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"markdown","source":"# Topsis Parameters","metadata":{}},{"cell_type":"code","source":"def calculate_redundancy(summaries):\n    \n    total_tokens = sum(len(summary.split()) for summary in summaries)\n    unique_tokens = len(set(token for summary in summaries for token in summary.split()))\n    redundancy_score = 1 - (unique_tokens / total_tokens)\n    \n    return redundancy_score","metadata":{"execution":{"iopub.status.busy":"2025-02-02T11:29:57.242733Z","iopub.execute_input":"2025-02-02T11:29:57.243301Z","iopub.status.idle":"2025-02-02T11:29:57.252389Z","shell.execute_reply.started":"2025-02-02T11:29:57.243273Z","shell.execute_reply":"2025-02-02T11:29:57.250779Z"},"trusted":true},"outputs":[],"execution_count":53},{"cell_type":"code","source":"def calc_bleu(actual_summary, predicted_summary):\n    actual_summaries_tokenized = [[ref.split()] for ref in actual_summaries]         # tokenizing the actual summary\n    pred_summaries_tokenized = [output.split() for output in pred_summaries]         # tokenizing the predicted summary\n    bleu_score = corpus_bleu(actual_summaries_tokenized, pred_summaries_tokenized)\n    return bleu_score\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calc_bert(actual_summary, predicted_summary):\n    P, R, F1 = score(actual_summary, predicted_summary, lang='en', verbose=False)     # returns Precision, Recall and F1 score\n    bert_score = F1.mean().item()  \n    return bert_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T11:29:57.268177Z","iopub.execute_input":"2025-02-02T11:29:57.268908Z","iopub.status.idle":"2025-02-02T11:29:57.288716Z","shell.execute_reply.started":"2025-02-02T11:29:57.268865Z","shell.execute_reply":"2025-02-02T11:29:57.287556Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"def calculate_rouge(actual_summary, predicted_summary):\n    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)         \n             \n    rouge1_scores = []\n    rouge2_scores = []\n    rougeL_scores = []\n\n    for pred, actual in zip(predicted_summary, actual_summary):\n        rouge_scores = rouge.score(pred, actual)                                               \n        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n        rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n\n            \n    rouge1 = sum(rouge1_scores) / len(rouge1_scores)                                      \n    rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n    rougeL = sum(rougeL_scores) / len(rougeL_scores)\n\n    return rouge1, rouge2, rougeL","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T11:29:57.289900Z","iopub.execute_input":"2025-02-02T11:29:57.290282Z","iopub.status.idle":"2025-02-02T11:29:57.316389Z","shell.execute_reply.started":"2025-02-02T11:29:57.290257Z","shell.execute_reply":"2025-02-02T11:29:57.314829Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"def get_metric(actual, predicted):\n    redundancy = calculate_redundancy(predicted)\n    rouge1, rouge2, rougeL = calculate_rouge(actual, predicted)\n    bert = calc_bert(actual, predicted)\n    bleu = calc_bleu(actual, predicted)\n    \n    return redundancy, bleu, bert, rouge1, rouge2, rougeL\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T11:29:57.319047Z","iopub.execute_input":"2025-02-02T11:29:57.319407Z","iopub.status.idle":"2025-02-02T11:29:57.328183Z","shell.execute_reply.started":"2025-02-02T11:29:57.319381Z","shell.execute_reply":"2025-02-02T11:29:57.327128Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"redundancy_score=[]\nbleu_score=[]\nbert_score=[]\nrouge1_score=[]\nrouge2_score=[]\nrougeL_score=[]","metadata":{"execution":{"iopub.status.busy":"2025-02-02T11:29:57.329185Z","iopub.execute_input":"2025-02-02T11:29:57.329464Z","iopub.status.idle":"2025-02-02T11:29:57.344534Z","shell.execute_reply.started":"2025-02-02T11:29:57.329442Z","shell.execute_reply":"2025-02-02T11:29:57.343262Z"},"trusted":true},"outputs":[],"execution_count":58},{"cell_type":"code","source":"df= df.sample(n=50, replace= False).reset_index(drop= True)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T11:31:24.773903Z","iopub.execute_input":"2025-02-02T11:31:24.774278Z","iopub.status.idle":"2025-02-02T11:31:24.782692Z","shell.execute_reply.started":"2025-02-02T11:31:24.774252Z","shell.execute_reply":"2025-02-02T11:31:24.780526Z"},"trusted":true},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"models= ['philschmid/bart-large-cnn-samsum',\n         'facebook/bart-large-cnn',\n         'philschmid/distilbart-cnn-12-6-samsum',\n         'knkarthick/MEETING_SUMMARY',\n         'google/bigbird-pegasus-large-arxiv']","metadata":{"execution":{"iopub.status.busy":"2025-02-02T11:31:27.076188Z","iopub.execute_input":"2025-02-02T11:31:27.077042Z","iopub.status.idle":"2025-02-02T11:31:27.082683Z","shell.execute_reply.started":"2025-02-02T11:31:27.077007Z","shell.execute_reply":"2025-02-02T11:31:27.080994Z"},"trusted":true},"outputs":[],"execution_count":60},{"cell_type":"code","source":"from transformers import pipeline\n\nfor Model in models:\n    warnings.filterwarnings(\"ignore\")\n    pipe = pipeline(\"summarization\", model=Model)\n    \n    print(\"\\nMODEL: \",Model)\n    print(pipe(df['dialogue'][0], max_length= 130, min_length=30, truncation= True))\n    \n    predictions= []\n    for i in range(0,len(df)):\n        warnings.filterwarnings(\"ignore\")\n        pred= pipe(df['dialogue'][i], max_length=130, min_length=30, truncation= True)[0]['summary_text']\n        predictions.append(pred)\n    \n    BLEU,BERT,Rouge_1,Rouge_2,Rouge_L,Redundancy = get_metric(df['summary'].tolist(), predictions)\n    \n    print('PARAMETERS')\n    print('BLEU Score: ',BLEU)\n    print('BERT Score: ',BERT)\n    print('Rouge-1 Score: ',Rouge_1)\n    print('Rouge-2 Score: ',Rouge_2)\n    print('Rouge-L Score: ',Rouge_L)\n    print('Redundancy Score: ',Redundancy)\n    \n    bleu_score.append(BLEU)\n    bert_score.append(BERT)\n    rouge1_score.append(Rouge_1)\n    rouge2_score.append(Rouge_2)\n    rougeL_score.append(Rouge_L)\n    redundancy_score.append(Redundancy)\n    \n    print('\\n')","metadata":{"execution":{"iopub.status.busy":"2025-02-02T11:31:42.944034Z","iopub.execute_input":"2025-02-02T11:31:42.944494Z","iopub.status.idle":"2025-02-02T12:28:14.150390Z","shell.execute_reply.started":"2025-02-02T11:31:42.944466Z","shell.execute_reply":"2025-02-02T12:28:14.148874Z"},"_kg_hide-output":true,"trusted":true},"outputs":[{"name":"stdout","text":"\nMODEL:  philschmid/bart-large-cnn-samsum\n[{'summary_text': \"Molly and Luca are having a silly season. Molly can't think and is frustrated. Luca tries to cheer her up.\"}]\n","output_type":"stream"},{"name":"stderr","text":"Your max_length is set to 130, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\nYour max_length is set to 130, but your input_length is only 55. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\nYour max_length is set to 130, but your input_length is only 109. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\nYour max_length is set to 130, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\nYour max_length is set to 130, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\nYour max_length is set to 130, but your input_length is only 65. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\nYour max_length is set to 130, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\nYour max_length is set to 130, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\nYour max_length is set to 130, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\nYour max_length is set to 130, but your input_length is only 123. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\nYour max_length is set to 130, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\nYour max_length is set to 130, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\nYour max_length is set to 130, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\nYour max_length is set to 130, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\nYour max_length is set to 130, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\nYour max_length is set to 130, but your input_length is only 104. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\nYour max_length is set to 130, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\nYour max_length is set to 130, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\nYour max_length is set to 130, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\nYour max_length is set to 130, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\nYour max_length is set to 130, but your input_length is only 90. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\nYour max_length is set to 130, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\nYour max_length is set to 130, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb086c4ccff440d59d7c23b85bf33472"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d873f554f8469cb989fcd9ca7e7ac2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f04676aa7a4e441988af84057839b2b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10340a4cd0824bcc85c6790e86dfbb07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94f2341e45b8454c9e2eede9ee459543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4773fa7776ee447b97ffc38bb84a362d"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"PARAMETERS\nBLEU Score:  0.5248425472358292\nBERT Score:  0.14330235645820502\nRouge-1 Score:  0.9124612212181091\nRouge-2 Score:  0.4897180815872998\nRouge-L Score:  0.2536939137548797\nRedundancy Score:  0.3850672857072429\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"857abe840c7d4fa59b33f68697e33ad0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34b36f07628f437b9f5ac2eaf5b9f85e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b24a9bb808c4ce7bbd6f6ab3c68669c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"330922be21fe451088d14706d15f514f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5b926b816024f3f998b647d7132d46f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f0947fad9f649039693928fa7350339"}},"metadata":{}},{"name":"stdout","text":"\nMODEL:  facebook/bart-large-cnn\n[{'summary_text': \"Molly: It's the silly season, isn't it? Luca: Yep. Just hang in there. Molly: I can't think anymore today!Luca: LOL! Molly: You can't let it get you down!\"}]\n","output_type":"stream"},{"name":"stderr","text":"Your max_length is set to 130, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\nYour max_length is set to 130, but your input_length is only 55. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\nYour max_length is set to 130, but your input_length is only 109. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\nYour max_length is set to 130, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\nYour max_length is set to 130, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\nYour max_length is set to 130, but your input_length is only 65. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\nYour max_length is set to 130, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\nYour max_length is set to 130, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\nYour max_length is set to 130, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\nYour max_length is set to 130, but your input_length is only 123. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\nYour max_length is set to 130, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\nYour max_length is set to 130, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\nYour max_length is set to 130, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\nYour max_length is set to 130, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\nYour max_length is set to 130, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\nYour max_length is set to 130, but your input_length is only 104. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\nYour max_length is set to 130, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\nYour max_length is set to 130, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\nYour max_length is set to 130, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\nYour max_length is set to 130, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\nYour max_length is set to 130, but your input_length is only 90. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\nYour max_length is set to 130, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\nYour max_length is set to 130, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"PARAMETERS\nBLEU Score:  0.4580858085808581\nBERT Score:  0.05664177239359781\nRouge-1 Score:  0.8735765218734741\nRouge-2 Score:  0.3337717848894025\nRouge-L Score:  0.11013806720926396\nRedundancy Score:  0.23914419749950994\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.85k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ac1d4294fb54b7a92ef9827a63e3374"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c733d6b8d8e1447f813a584bc09bbd38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62902d11283246f3995f3bad97258513"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84c48809aa7149449bfc15f341417ab9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93f953bd83d9444d92964d6a1ff435e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90b3eb222ca941c89bc18821ef335df7"}},"metadata":{}},{"name":"stdout","text":"\nMODEL:  philschmid/distilbart-cnn-12-6-samsum\n[{'summary_text': \"Molly can't think anymore today. It's the silly season. Luca reminds Molly not to let it get her down. \"}]\n","output_type":"stream"},{"name":"stderr","text":"Your max_length is set to 130, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\nYour max_length is set to 130, but your input_length is only 55. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\nYour max_length is set to 130, but your input_length is only 109. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\nYour max_length is set to 130, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\nYour max_length is set to 130, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\nYour max_length is set to 130, but your input_length is only 65. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\nYour max_length is set to 130, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\nYour max_length is set to 130, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\nYour max_length is set to 130, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\nYour max_length is set to 130, but your input_length is only 123. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\nYour max_length is set to 130, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\nYour max_length is set to 130, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\nYour max_length is set to 130, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\nYour max_length is set to 130, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\nYour max_length is set to 130, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\nYour max_length is set to 130, but your input_length is only 104. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\nYour max_length is set to 130, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\nYour max_length is set to 130, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\nYour max_length is set to 130, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\nYour max_length is set to 130, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\nYour max_length is set to 130, but your input_length is only 90. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\nYour max_length is set to 130, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\nYour max_length is set to 130, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"PARAMETERS\nBLEU Score:  0.5116788321167883\nBERT Score:  0.13932229438644483\nRouge-1 Score:  0.9088510870933533\nRouge-2 Score:  0.47080047049510815\nRouge-L Score:  0.23880643154337491\nRedundancy Score:  0.36200919848714785\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.59k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"901685bc78ac4770bcedb6c466be8809"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b22f1cfdb74b4fe0bd2080fde06479b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/337 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cfaa5bf5706483b99e37e4d3fb16e96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a4c93b51e95444fa048224053e03391"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4b4ffa07a344e24bc1657761b1018a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc12e655407a4449b73fe5541c4f7883"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef144d2bb6314b87acda06d4c0dfb70c"}},"metadata":{}},{"name":"stdout","text":"\nMODEL:  knkarthick/MEETING_SUMMARY\n[{'summary_text': \"It's the silly season and Molly is stressed out because she can't think of what to do next. Luca tries to cheer her up.\"}]\n","output_type":"stream"},{"name":"stderr","text":"Your max_length is set to 130, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\nYour max_length is set to 130, but your input_length is only 55. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\nYour max_length is set to 130, but your input_length is only 109. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=54)\nYour max_length is set to 130, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\nYour max_length is set to 130, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\nYour max_length is set to 130, but your input_length is only 65. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\nYour max_length is set to 130, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\nYour max_length is set to 130, but your input_length is only 124. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=62)\nYour max_length is set to 130, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\nYour max_length is set to 130, but your input_length is only 123. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\nYour max_length is set to 130, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\nYour max_length is set to 130, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\nYour max_length is set to 130, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\nYour max_length is set to 130, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\nYour max_length is set to 130, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\nYour max_length is set to 130, but your input_length is only 104. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=52)\nYour max_length is set to 130, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\nYour max_length is set to 130, but your input_length is only 31. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=15)\nYour max_length is set to 130, but your input_length is only 42. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\nYour max_length is set to 130, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\nYour max_length is set to 130, but your input_length is only 90. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\nYour max_length is set to 130, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\nYour max_length is set to 130, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"PARAMETERS\nBLEU Score:  0.5191370911621433\nBERT Score:  0.13732998877543318\nRouge-1 Score:  0.9100101590156555\nRouge-2 Score:  0.4683979476855605\nRouge-L Score:  0.23578188575238948\nRedundancy Score:  0.354625858941433\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7335dcf956ef4f4bb0744c003729f068"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.31G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee207328958440e78d55019d5eed1933"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/232 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bff6163feb949cea847a6aacb607b94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2c48fc4f0734a398b20cb15f0e4e267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.92M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f63ab3e7d4c4bc19c058cb2faacf039"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/3.51M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b678205a74a43a48902ba5782573c6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/775 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"355d06c3c69f44cd83b0ed0cc8fb19f4"}},"metadata":{}},{"name":"stderr","text":"Your max_length is set to 130, but your input_length is only 114. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\nAttention type 'block_sparse' is not possible if sequence_length: 114 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n","output_type":"stream"},{"name":"stdout","text":"\nMODEL:  google/bigbird-pegasus-large-arxiv\n","output_type":"stream"},{"name":"stderr","text":"Your max_length is set to 130, but your input_length is only 114. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n","output_type":"stream"},{"name":"stdout","text":"[{'summary_text': 'it has been shown that the laws of thermodynamics can be used to predict the outcome of many physical experiments .<n> here we show that the laws of thermodynamics can also be used to predict the outcome of some experiments .<n> we do this by showing that the laws of thermodynamics can be used to predict the outcome of experiments in which the experimenter is subjected to some external perturbation .<n> we do this by showing that the predictions of the laws of thermodynamics can be used to predict the outcome of experiments in which the experimenter is subjected to some external perturbation .<n> we do this by showing that the predictions of the laws'}]\n","output_type":"stream"},{"name":"stderr","text":"Your max_length is set to 130, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\nYour max_length is set to 130, but your input_length is only 69. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\nYour max_length is set to 130, but your input_length is only 41. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\nYour max_length is set to 130, but your input_length is only 101. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\nYour max_length is set to 130, but your input_length is only 14. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\nYour max_length is set to 130, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\nYour max_length is set to 130, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\nYour max_length is set to 130, but your input_length is only 116. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=58)\nYour max_length is set to 130, but your input_length is only 32. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\nYour max_length is set to 130, but your input_length is only 98. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\nYour max_length is set to 130, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\nYour max_length is set to 130, but your input_length is only 95. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\nYour max_length is set to 130, but your input_length is only 33. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\nYour max_length is set to 130, but your input_length is only 81. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\nYour max_length is set to 130, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\nYour max_length is set to 130, but your input_length is only 72. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\nYour max_length is set to 130, but your input_length is only 75. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\nYour max_length is set to 130, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\nYour max_length is set to 130, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\nYour max_length is set to 130, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\nYour max_length is set to 130, but your input_length is only 84. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\nYour max_length is set to 130, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\nYour max_length is set to 130, but your input_length is only 33. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\nYour max_length is set to 130, but your input_length is only 50. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=25)\nYour max_length is set to 130, but your input_length is only 76. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\nYour max_length is set to 130, but your input_length is only 63. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=31)\nYour max_length is set to 130, but your input_length is only 126. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=63)\nYour max_length is set to 130, but your input_length is only 60. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"PARAMETERS\nBLEU Score:  0.8732200275608636\nBERT Score:  0.06031464116282594\nRouge-1 Score:  0.792246401309967\nRouge-2 Score:  0.04762497445521983\nRouge-L Score:  0.0007775151037562385\nRedundancy Score:  0.04297484876934316\n\n\n","output_type":"stream"}],"execution_count":62},{"cell_type":"markdown","source":"**Scores**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nscores= [bleu_score,bert_score,\n    rouge1_score,\n    rouge2_score,\n    rougeL_score,\n    redundancy_score]\nfor score in scores:\n    for i in range(len(score)):\n        score[i]= np.round(score[i],3)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T12:34:30.683294Z","iopub.execute_input":"2025-02-02T12:34:30.683716Z","iopub.status.idle":"2025-02-02T12:34:30.691045Z","shell.execute_reply.started":"2025-02-02T12:34:30.683686Z","shell.execute_reply":"2025-02-02T12:34:30.689683Z"},"trusted":true},"outputs":[],"execution_count":75},{"cell_type":"code","source":"df_topsis= pd.DataFrame({\n    'Model': models,\n    'BLEU': bleu_score,\n    'BERT': bert_score,\n    'Rouge-1': rouge1_score,\n    'Rouge-2': rouge2_score,\n    'Rouge-L': rougeL_score,\n    'Redundancy': redundancy_score\n})","metadata":{"execution":{"iopub.status.busy":"2025-02-02T12:34:33.229163Z","iopub.execute_input":"2025-02-02T12:34:33.230777Z","iopub.status.idle":"2025-02-02T12:34:33.237153Z","shell.execute_reply.started":"2025-02-02T12:34:33.230497Z","shell.execute_reply":"2025-02-02T12:34:33.236002Z"},"trusted":true},"outputs":[],"execution_count":76},{"cell_type":"code","source":"df_topsis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:34:35.598120Z","iopub.execute_input":"2025-02-02T12:34:35.598492Z","iopub.status.idle":"2025-02-02T12:34:35.614834Z","shell.execute_reply.started":"2025-02-02T12:34:35.598465Z","shell.execute_reply":"2025-02-02T12:34:35.613400Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"                                   Model   BLEU   BERT  Rouge-1  Rouge-2  \\\n0       philschmid/bart-large-cnn-samsum  0.525  0.143    0.912    0.490   \n1                facebook/bart-large-cnn  0.458  0.057    0.874    0.334   \n2  philschmid/distilbart-cnn-12-6-samsum  0.512  0.139    0.909    0.471   \n3             knkarthick/MEETING_SUMMARY  0.519  0.137    0.910    0.468   \n4     google/bigbird-pegasus-large-arxiv  0.873  0.060    0.792    0.048   \n\n   Rouge-L  Redundancy  \n0    0.254       0.385  \n1    0.110       0.239  \n2    0.239       0.362  \n3    0.236       0.355  \n4    0.001       0.043  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>BLEU</th>\n      <th>BERT</th>\n      <th>Rouge-1</th>\n      <th>Rouge-2</th>\n      <th>Rouge-L</th>\n      <th>Redundancy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>philschmid/bart-large-cnn-samsum</td>\n      <td>0.525</td>\n      <td>0.143</td>\n      <td>0.912</td>\n      <td>0.490</td>\n      <td>0.254</td>\n      <td>0.385</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>facebook/bart-large-cnn</td>\n      <td>0.458</td>\n      <td>0.057</td>\n      <td>0.874</td>\n      <td>0.334</td>\n      <td>0.110</td>\n      <td>0.239</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>philschmid/distilbart-cnn-12-6-samsum</td>\n      <td>0.512</td>\n      <td>0.139</td>\n      <td>0.909</td>\n      <td>0.471</td>\n      <td>0.239</td>\n      <td>0.362</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>knkarthick/MEETING_SUMMARY</td>\n      <td>0.519</td>\n      <td>0.137</td>\n      <td>0.910</td>\n      <td>0.468</td>\n      <td>0.236</td>\n      <td>0.355</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>google/bigbird-pegasus-large-arxiv</td>\n      <td>0.873</td>\n      <td>0.060</td>\n      <td>0.792</td>\n      <td>0.048</td>\n      <td>0.001</td>\n      <td>0.043</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":77},{"cell_type":"markdown","source":"# Topsis","metadata":{}},{"cell_type":"code","source":"weights= [0.2,0.15,0.20,0.20,0.15,0.1]         \nimpacts= ['+','+','+','+','+','-']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:34:39.141009Z","iopub.execute_input":"2025-02-02T12:34:39.141720Z","iopub.status.idle":"2025-02-02T12:34:39.147086Z","shell.execute_reply.started":"2025-02-02T12:34:39.141691Z","shell.execute_reply":"2025-02-02T12:34:39.145338Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"def normalize(matrix):\n    norm_matrix = matrix / np.sqrt(np.sum(matrix**2, axis=0))                    # normalize the matrix\n    return norm_matrix\n\ndef weighted_normalize(norm_matrix, weights):\n    weighted_norm_matrix = norm_matrix * weights                                 # calculate the weighted normalized matrix\n    return weighted_norm_matrix\n\ndef ideal_best_worst(weighted_norm_matrix, impacts):\n    ideal_solution = np.max(weighted_norm_matrix, axis=0) * impacts              # calculate the ideal_best and ideal_worst solutions\n    ideal_worst_solution = np.min(weighted_norm_matrix, axis=0) * impacts\n    return ideal_solution, ideal_worst_solution\n\ndef euclidean_distances(weighted_norm_matrix, ideal_solution, ideal_worst_solution):\n    dist_to_ideal = np.sqrt(np.sum((weighted_norm_matrix - ideal_solution)**2, axis=1))           # Calculate the Euclidean distances to the ideal_best and ideal_worst solutions.\n    dist_to_ideal_worst = np.sqrt(np.sum((weighted_norm_matrix - ideal_worst_solution)**2, axis=1))\n    return dist_to_ideal, dist_to_ideal_worst\n\ndef performance_score(dist_to_ideal, dist_to_ideal_worst):\n    score = dist_to_ideal_worst / (dist_to_ideal + dist_to_ideal_worst)            # calculate the topsis score for each model\n    return score\n\ndef topsis(matrix, weights, impacts):                                              # perform TOPSIS analysis\n    # Step 1: Normalize the decision matrix\n    norm_matrix = normalize(matrix)\n    \n    # Step 2: Calculate the weighted normalized decision matrix\n    weighted_norm_matrix = weighted_normalize(norm_matrix, weights)\n    ideal_solution, ideal_worst_solution = ideal_best_worst(weighted_norm_matrix, impacts)\n    \n    # Step 4: Calculate the Euclidean distances to the ideal_best and ideal_worst solutions\n    dist_to_ideal, dist_to_ideal_worst = euclidean_distances(weighted_norm_matrix, ideal_solution, ideal_worst_solution)\n    \n    # Step 5: Calculate the performance score for each alternative/model\n    score = performance_score(dist_to_ideal, dist_to_ideal_worst)\n    \n    # Step 6: Rank the alternatives/models based on their performance scores\n    sorted_indices = np.argsort(score)[::-1]                                       # Indices of scores sorted in descending order\n    rankings = np.empty_like(sorted_indices)                                       # Create an empty array to store rankings\n    rankings[sorted_indices] = np.arange(len(score)) + 1                           # Assign ranks\n    \n    return score, rankings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:34:40.986792Z","iopub.execute_input":"2025-02-02T12:34:40.987285Z","iopub.status.idle":"2025-02-02T12:34:40.999237Z","shell.execute_reply.started":"2025-02-02T12:34:40.987251Z","shell.execute_reply":"2025-02-02T12:34:40.997989Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"df_metrics= df_topsis.drop('Model',axis=1)\nimpacts_as_integers = [1 if impact == '+' else -1 for impact in impacts]\ntopsis_score, rankings = topsis(df_metrics, weights, impacts_as_integers)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:34:44.091303Z","iopub.execute_input":"2025-02-02T12:34:44.092402Z","iopub.status.idle":"2025-02-02T12:34:44.103853Z","shell.execute_reply.started":"2025-02-02T12:34:44.092355Z","shell.execute_reply":"2025-02-02T12:34:44.102306Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"for i in range(len(topsis_score)):\n    topsis_score[i] = np.round(topsis_score[i], 3)\n    \ndf_topsis['TOPSIS Score'] = topsis_score\ndf_topsis['TOPSIS Rank'] = rankings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:34:46.148998Z","iopub.execute_input":"2025-02-02T12:34:46.149385Z","iopub.status.idle":"2025-02-02T12:34:46.156599Z","shell.execute_reply.started":"2025-02-02T12:34:46.149358Z","shell.execute_reply":"2025-02-02T12:34:46.155417Z"}},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":"# Final Result and Ranking","metadata":{}},{"cell_type":"code","source":"df_topsis","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-02T12:34:48.273712Z","iopub.execute_input":"2025-02-02T12:34:48.274102Z","iopub.status.idle":"2025-02-02T12:34:48.291046Z","shell.execute_reply.started":"2025-02-02T12:34:48.274074Z","shell.execute_reply":"2025-02-02T12:34:48.289746Z"}},"outputs":[{"execution_count":82,"output_type":"execute_result","data":{"text/plain":"                                   Model   BLEU   BERT  Rouge-1  Rouge-2  \\\n0       philschmid/bart-large-cnn-samsum  0.525  0.143    0.912    0.490   \n1                facebook/bart-large-cnn  0.458  0.057    0.874    0.334   \n2  philschmid/distilbart-cnn-12-6-samsum  0.512  0.139    0.909    0.471   \n3             knkarthick/MEETING_SUMMARY  0.519  0.137    0.910    0.468   \n4     google/bigbird-pegasus-large-arxiv  0.873  0.060    0.792    0.048   \n\n   Rouge-L  Redundancy  TOPSIS Score  TOPSIS Rank  \n0    0.254       0.385         0.555            1  \n1    0.110       0.239         0.386            4  \n2    0.239       0.362         0.547            2  \n3    0.236       0.355         0.546            3  \n4    0.001       0.043         0.291            5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>BLEU</th>\n      <th>BERT</th>\n      <th>Rouge-1</th>\n      <th>Rouge-2</th>\n      <th>Rouge-L</th>\n      <th>Redundancy</th>\n      <th>TOPSIS Score</th>\n      <th>TOPSIS Rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>philschmid/bart-large-cnn-samsum</td>\n      <td>0.525</td>\n      <td>0.143</td>\n      <td>0.912</td>\n      <td>0.490</td>\n      <td>0.254</td>\n      <td>0.385</td>\n      <td>0.555</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>facebook/bart-large-cnn</td>\n      <td>0.458</td>\n      <td>0.057</td>\n      <td>0.874</td>\n      <td>0.334</td>\n      <td>0.110</td>\n      <td>0.239</td>\n      <td>0.386</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>philschmid/distilbart-cnn-12-6-samsum</td>\n      <td>0.512</td>\n      <td>0.139</td>\n      <td>0.909</td>\n      <td>0.471</td>\n      <td>0.239</td>\n      <td>0.362</td>\n      <td>0.547</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>knkarthick/MEETING_SUMMARY</td>\n      <td>0.519</td>\n      <td>0.137</td>\n      <td>0.910</td>\n      <td>0.468</td>\n      <td>0.236</td>\n      <td>0.355</td>\n      <td>0.546</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>google/bigbird-pegasus-large-arxiv</td>\n      <td>0.873</td>\n      <td>0.060</td>\n      <td>0.792</td>\n      <td>0.048</td>\n      <td>0.001</td>\n      <td>0.043</td>\n      <td>0.291</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":82},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}